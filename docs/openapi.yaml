openapi: 3.0.0
info:
  title: Anthropic API
  description: APIs for sampling from and fine-tuning language models
  version: '1.0.0'
servers:
  - url: https://api.anthropic.com/v1
tags:
- name: Anthropic
  description: The Anthropic REST API
paths:
  /complete:
    post:
      operationId: complete
      tags:
      - Anthropic
      summary: Creates a completion for the provided prompt and parameters.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateCompletionRequest'
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateCompletionResponse'
      x-oaiMeta:
        name: Create completion
        group: completions
        path: create
        examples:
          curl: |
            curl https://api.anthropic.com/v1/complete \
              --header 'accept: application/json' \
              --header 'anthropic-version: 2023-06-01' \
              --header 'content-type: application/json' \
              --header 'x-api-key: $ANTHROPIC_API_KEY' \
              --data '
              {
                "model": "claude-2",
                "prompt": "\n\nHuman: Hello, world!\n\nAssistant:",
                "max_tokens_to_sample": 256
              }'
        parameters: |
          {
            "model": "claude-2",
            "prompt": "\n\nHuman: Hello, world!\n\nAssistant:",
            "max_tokens_to_sample": 256
          }
        response: |
          {
            "completion": " Hello! My name is Claude.",
            "stop_reason": "stop_sequence",
            "model": "claude-2"
          }
          
components:
  schemas:
    Error:
      type: object
      properties:
        type:
          type: string
          nullable: false
        message:
          type: string
          nullable: false
      required:
        - type
        - message
    
    ErrorResponse:
      type: object
      properties:
        error:
          $ref: '#/components/schemas/Error'
      required:
        - error

    CreateCompletionRequest:
      type: object
      properties:
        model:
          description: &model_description |
            The model that will complete your prompt.
            As we improve Claude, we develop new versions of it that you can query.
            This parameter controls which version of Claude answers your request.
            Right now we are offering two model families: Claude, and Claude Instant.
            You can use them by setting model to "claude-2" or "claude-instant-1", respectively.
            See models for additional details.
          oneOf:
            - type: string
            - type: string
              enum: ["claude-2","claude-2.0","claude-instant-1","claude-instant-1.1"]
          x-oaiTypeLabel: string
        prompt:
          description: &completions_prompt_description |
            The prompt that you want Claude to complete.

            For proper response generation you will need to format your prompt as follows:
            \n\nHuman: ${userQuestion}\n\nAssistant:
            See our comments on prompts for more context.
          default: '<|endoftext|>'
          nullable: true
          oneOf:
            - type: string
              default: ''
              example: "This is a test."
            - type: array
              items:
                type: string
                default: ''
                example: "This is a test."
            - type: array
              minItems: 1
              items:
                type: integer
              example: "[1212, 318, 257, 1332, 13]"
            - type: array
              minItems: 1
              items:
                type: array
                minItems: 1
                items:
                  type: integer
              example: "[[1212, 318, 257, 1332, 13]]"
        max_tokens_to_sample:
          type: integer
          minimum: 1
          default: 256
          example: 256
          nullable: true
          description: &completions_max_tokens_to_sample_description |
            The maximum number of tokens to generate before stopping.

            Note that our models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.
        temperature:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: &completions_temperature_description |
            Amount of randomness injected into the response.
            
            Defaults to 1. Ranges from 0 to 1. Use temp closer to 0 for analytical / multiple choice, and closer to 1 for creative and generative tasks.
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: &completions_top_p_description |
            Use nucleus sampling.
  
            In nucleus sampling, we compute the cumulative distribution over all the options 
            for each subsequent token in decreasing probability order and cut it off once 
            it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.
        top_k:
          type: number
          minimum: 0
          default: 5
          example: 5
          nullable: true
          description: &completions_top_k_description |
            Only sample from the top K options for each subsequent token.
  
            Used to remove "long tail" low probability responses. Learn more technical details here.
        stream:
          description: >
            Whether to incrementally stream the response using server-sent events.

            See this guide to SSE events for details.type: boolean
          nullable: true
          default: false
        stop_sequences:
          description: &completions_stop_description >
            Sequences that will cause the model to stop generating completion text.
            
            Our models stop on "\n\nHuman:", and may include additional built-in stop sequences in the future.
            By providing the stop_sequences parameter, you may include additional strings that will cause the model to stop generating.
          default: null
          nullable: true
          oneOf:
            - type: string
              default: <|endoftext|>
              example: "\n"
              nullable: true
            - type: array
              minItems: 1
              maxItems: 4
              items:
                type: string
                example: '["\n"]'
        metadata: &end_user_param_configuration
          type: object
          properties:
            user_id:
              type: string
              example: 13803d75-b4b5-4c3e-b2a2-6f21399b021b
              description: |
                An external identifier for the user who is associated with the request.
                
                This should be a uuid, hash value, or other opaque identifier. Anthropic may use this id to help detect abuse. 
                Do not include any identifying information such as name, email address, or phone number.
          description: |
            An object describing metadata about the request.
      required:
        - model
        - prompt
        - max_tokens_to_sample
    
    CreateCompletionResponse:
      type: object
      properties:
        stop_reason:
          type: string
          enum: ["stop_sequence","max_tokens"]
          description: |
            The reason that we stopped sampling.

            This may be one the following values:
            
            "stop_sequence": we reached a stop sequence â€” either provided by you via the stop_sequences parameter, or a stop sequence built into the model
            "max_tokens": we exceeded max_tokens_to_sample or the model's maximum
        model:
          type: string
          description: |
            The model that performed the completion.
        completion:
          type: string
          description: |
            The resulting completion up to and excluding the stop sequences.
      required: 
        - completion
        - stop_reason
        - model

    ChatCompletionStreamResponseDelta:
      type: object
      properties:
        role:
          type: string
          enum: ["system", "user", "assistant", "function"]
          description: The role of the author of this message.
        content:
          type: string
          description: The contents of the chunk message.
          nullable: true
        function_call:
          type: object
          description: The name and arguments of a function that should be called, as generated by the model.
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.

x-oaiMeta:
  groups:
    - id: completions
      title: Completions
      description: |
        Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position. Note: We recommend most users use our Chat Completions API. [Learn more](/docs/deprecations/2023-07-06-gpt-and-embeddings)
